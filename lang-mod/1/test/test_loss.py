from __future__ import print_function
from __future__ import division

import tensorflow as tf
import numpy as np
import model

'''
[[ 0.03945358 -0.0444226 ]
 [ 0.02318096  0.00571191]
 [ 0.00215099  0.00061758]
 [-0.04630127 -0.03907587]
 [ 0.02477768 -0.02004011]
 [ 0.00562305  0.03217997]
 [ 0.0229703  -0.03174545]
 [-0.04438719 -0.00687166]
 [ 0.03753287 -0.00732407]
 [ 0.02020464 -0.04932442]
 [-0.02919442 -0.04751461]
 [-0.04881425  0.03355291]
 [ 0.01543792 -0.01663084]
 [-0.00563183 -0.01765734]
 [-0.02370799  0.03013503]
 [-0.01040227  0.01394242]
 [-0.02957147  0.01905343]
 [ 0.01896143  0.0437004 ]
 [-0.0488611  -0.02024952]
 [-0.03427717 -0.02002436]]
[[-0.04582611  0.03165361  0.03341099  0.02498396 -0.00405747]
 [ 0.04609683 -0.02605636  0.03114619 -0.04617096  0.01960472]
 [ 0.01715769  0.04887364 -0.04406923  0.02113665 -0.01745777]
 [ 0.01267737  0.04962686  0.03808175  0.00741551  0.00754201]
 [ 0.00734122  0.00167065 -0.00884858  0.02363003  0.02987597]
 [-0.01590672 -0.00353671 -0.0046834  -0.02397486 -0.01590803]
 [-0.02862644 -0.02824073  0.01624198 -0.0225911   0.03864351]
 [-0.03349948  0.02139851  0.01027939  0.01095601 -0.0151518 ]
 [-0.01234832  0.03841607  0.04164791  0.04757022  0.02577851]
 [-0.01668056 -0.04694973  0.00422466 -0.02944108 -0.02543925]
 [ 0.03240678  0.01726907 -0.0194487   0.00952698 -0.0277692 ]
 [-0.00158513  0.02107752 -0.01342812 -0.00992333  0.04881607]
 [ 0.01516276 -0.01148018 -0.02377852  0.02116852  0.00681743]
 [-0.03672289  0.00992345  0.00807212 -0.02994791  0.01511351]
 [-0.0141693  -0.00466816 -0.01336293  0.04354934 -0.04254397]
 [ 0.01684963  0.04686322 -0.02303303  0.00437417 -0.02925827]
 [-0.01778433  0.00464271  0.02053557 -0.03296186 -0.00603393]
 [-0.03159004  0.02571815 -0.01076855  0.02767207  0.02036933]
 [ 0.02858665  0.00938568 -0.0342062   0.03908044  0.02502953]
 [-0.04824239  0.00612401  0.01264863  0.03672833  0.01885514]]
[ 0.03002262  0.03607186 -0.03585084 -0.02389173  0.04190451  0.01144202
  0.00309152 -0.01952825 -0.04142651  0.01618458 -0.0224176  -0.03141491
 -0.04748542  0.02265899 -0.02689984 -0.03372463  0.00532304  0.0238619
  0.03720967  0.01196872]

Torch TOP_C and TOP_H

TOP_C:  0.01 *
 -1.6705  0.3561  1.1811  1.9170  0.6607
 -1.6712  0.3541  1.1826  1.9134  0.6582
 -1.6693  0.3587  1.1812  1.9196  0.6624
 -1.6693  0.3571  1.1862  1.9131  0.6574
[torch.DoubleTensor of size 4x5]

TOP_H:  0.001 *
 -8.4013  1.7818  5.8529  9.3769  3.3270
 -8.4050  1.7717  5.8601  9.3598  3.3142
 -8.3955  1.7945  5.8531  9.3896  3.3355
 -8.3960  1.7865  5.8777  9.3582  3.3101
[torch.DoubleTensor of size 4x5]


TOP_C:  0.01 *
 -2.5088  0.5292  1.7895  2.8844  1.0299
 -2.5093  0.5281  1.7901  2.8825  1.0286
 -2.5082  0.5318  1.7857  2.8908  1.0346
 -2.5078  0.5304  1.7929  2.8820  1.0278
[torch.DoubleTensor of size 4x5]

TOP_H:  0.01 *
 -1.2616  0.2648  0.8869  1.4114  0.5185
 -1.2618  0.2642  0.8871  1.4104  0.5178
 -1.2612  0.2661  0.8850  1.4145  0.5209
 -1.2611  0.2654  0.8886  1.4102  0.5174
[torch.DoubleTensor of size 4x5]


TOP_C:  0.01 *
 -2.9315  0.6096  2.1031  3.3684  1.2311
 -2.9293  0.6146  2.1030  3.3735  1.2344
 -2.9300  0.6146  2.0974  3.3795  1.2391
 -2.9291  0.6152  2.1020  3.3749  1.2355
[torch.DoubleTensor of size 4x5]

TOP_H:  0.01 *
 -1.4739  0.3050  1.0423  1.6485  0.6197
 -1.4728  0.3075  1.0423  1.6509  0.6214
 -1.4731  0.3076  1.0396  1.6539  0.6238
 -1.4727  0.3078  1.0418  1.6516  0.6220
[torch.DoubleTensor of size 4x5]




'''

LSTM_W = np.array([
 [ 0.03945358,  0.02318096,  0.00215099, -0.04630127,  0.02477768,  0.00562305,
   0.0229703 , -0.04438719,  0.03753287,  0.02020464, -0.02919442, -0.04881425,
   0.01543792, -0.00563183, -0.02370799, -0.01040227, -0.02957147,  0.01896143,
  -0.0488611 , -0.03427717],
 [-0.0444226 ,  0.00571191,  0.00061758, -0.03907587, -0.02004011,  0.03217997,
  -0.03174545, -0.00687166, -0.00732407, -0.04932442, -0.04751461,  0.03355291,
  -0.01663084, -0.01765734,  0.03013503,  0.01394242,  0.01905343,  0.0437004,
  -0.02024952, -0.02002436],
 [-0.04582611,  0.04609683,  0.01715769,  0.01267737,  0.00734122, -0.01590672,
  -0.02862644, -0.03349948, -0.01234832, -0.01668056,  0.03240678, -0.00158513,
   0.01516276, -0.03672289, -0.0141693 ,  0.01684963, -0.01778433, -0.03159004,
   0.02858665, -0.04824239],
 [ 0.03165361, -0.02605636,  0.04887364,  0.04962686,  0.00167065, -0.00353671,
  -0.02824073,  0.02139851,  0.03841607, -0.04694973,  0.01726907,  0.02107752,
  -0.01148018,  0.00992345, -0.00466816,  0.04686322,  0.00464271,  0.02571815,
   0.00938568,  0.00612401],
 [ 0.03341099,  0.03114619, -0.04406923,  0.03808175, -0.00884858, -0.0046834,
   0.01624198,  0.01027939,  0.04164791,  0.00422466, -0.0194487 , -0.01342812,
  -0.02377852,  0.00807212, -0.01336293, -0.02303303,  0.02053557, -0.01076855,
  -0.0342062 ,  0.01264863],
 [ 0.02498396, -0.04617096,  0.02113665,  0.00741551,  0.02363003, -0.02397486,
  -0.0225911 ,  0.01095601,  0.04757022, -0.02944108,  0.00952698, -0.00992333,
   0.02116852, -0.02994791,  0.04354934,  0.00437417, -0.03296186,  0.02767207,
   0.03908044,  0.03672833],
 [-0.00405747,  0.01960472, -0.01745777,  0.00754201,  0.02987597, -0.01590803,
   0.03864351, -0.0151518 ,  0.02577851, -0.02543925, -0.0277692 ,  0.04881607,
   0.00681743,  0.01511351, -0.04254397, -0.02925827, -0.00603393,  0.02036933,
   0.02502953,  0.01885514]
])

LSTM_B = np.array(
[ 0.03002262,  0.03607186, -0.03585084, -0.02389173,  0.04190451,  0.01144202,
  0.00309152, -0.01952825, -0.04142651,  0.01618458, -0.0224176 , -0.03141491,
 -0.04748542,  0.02265899, -0.02689984, -0.03372463,  0.00532304,  0.0238619,
  0.03720967,  0.01196872
])

INPUT_CNN = np.array([
 [[-0.04201929,  0.02275813],
  [-0.04060676,  0.02283999],
  [-0.04333816,  0.02333505],
  [-0.04131923,  0.02480407]],

 [[-0.04124087,  0.02429205],
  [-0.04117644,  0.02419558],
  [-0.04282973,  0.02318067],
  [-0.04131923,  0.02480407]],

 [[-0.03877186,  0.0243939 ],
  [-0.04173752,  0.02552123],
  [-0.04168687,  0.02385954],
  [-0.04201929,  0.02454825]]]).transpose( (1, 0, 2) )

assert LSTM_W.shape == (7, 20)
new_lstm_w = np.zeros((7,20), dtype=np.float32)
new_lstm_w[:,0:5]   = LSTM_W[:,0:5]
new_lstm_w[:,5:10]  = LSTM_W[:,15:20]
new_lstm_w[:,10:15] = LSTM_W[:,10:15]
new_lstm_w[:,15:20] = LSTM_W[:,5:10]
LSTM_W = new_lstm_w

new_lstm_b = np.zeros((20,), dtype=np.float32)
new_lstm_b[0:5]   = LSTM_B[0:5]
new_lstm_b[5:10]  = LSTM_B[15:20]
new_lstm_b[10:15] = LSTM_B[10:15]
new_lstm_b[15:20] = LSTM_B[5:10]
LSTM_B = new_lstm_b

inp = INPUT_CNN[0, 0, :]
print(inp)  # first batch, first unroll step

inp_h = np.hstack([inp, np.zeros(5)])
prev_c = np.zeros(5)

all_inps = np.dot(inp_h, LSTM_W) + LSTM_B
print('ALL_INPS:', all_inps)

i, j, f, o = np.split(all_inps, 4)
print(i, j, f, o)

def sigmoid(x):
    return 1. / (1. + np.exp(-x))

new_c = prev_c * sigmoid(f) + sigmoid(i) * np.tanh(j)
print('NEW_C:', new_c)

new_h = np.tanh(new_c) * sigmoid(o)
print('NEW_H:', new_h)

'''
next_c: [-0.01671056  0.00356125  0.01181377  0.01917946  0.00660749]
next_h: [-0.00840437  0.00178187  0.00585398  0.00938162  0.00332717]
'''

'''
[  1.88394852e-04   4.18517351e-04   4.94887555e-05   3.97929451e-04
   2.45719528e-04]
'''

'''
[array([[-0.00840133,  0.00178184,  0.00585286,  0.00937691,  0.00332699],
       [-0.00840504,  0.00177166,  0.00586006,  0.00935978,  0.00331423],
       [-0.00839551,  0.0017945 ,  0.00585306,  0.00938957,  0.00333546],
       [-0.00839595,  0.00178647,  0.0058777 ,  0.00935818,  0.00331012]], dtype=float32), array([[-0.0126155 ,  0.00264827,  0.00886869,  0.01411371,  0.00518486],
       [-0.01261795,  0.00264249,  0.00887132,  0.01410431,  0.00517832],
       [-0.01261209,  0.00266095,  0.00885007,  0.01414492,  0.00520893],
       [-0.01261059,  0.00265393,  0.00888564,  0.01410206,  0.00517435]], dtype=float32), array([[-0.01473925,  0.00305038,  0.01042287,  0.01648509,  0.00619734],
       [-0.01472822,  0.00307533,  0.01042284,  0.01650903,  0.00621392],
       [-0.01473146,  0.00307552,  0.01039554,  0.01653865,  0.00623778],
       [-0.01472719,  0.00307848,  0.01041825,  0.01651621,  0.00621954]], dtype=float32)]
'''

from read_param_init import SOFTMAX_W, SOFTMAX_B


Y = np.array([[   2,    3,    4],
       [  32,  429, 7408],
       [3078,   64,   35],
       [  27,   48,  395]], dtype=np.int32)

class TestRNN(tf.test.TestCase):

    def model(self):
        return model.inference_graph(char_vocab_size=51, word_vocab_size=10000,
                        char_embed_size=3, batch_size=4, num_highway_layers=0,
                        num_rnn_layers=1, rnn_size=5, max_word_length=11,
                        kernels= [2], kernel_features=[2], num_unroll_steps=3,
                        dropout=0.0)

    def test(self):

        with self.test_session() as sess:

            m = self.model()
            loss = model.loss_graph(m.logits, batch_size=4, num_unroll_steps=3)

            rnn_outputs = [
                np.array([[-0.00840133,  0.00178184,  0.00585286,  0.00937691,  0.00332699],
                       [-0.00840504,  0.00177166,  0.00586006,  0.00935978,  0.00331423],
                       [-0.00839551,  0.0017945 ,  0.00585306,  0.00938957,  0.00333546],
                       [-0.00839595,  0.00178647,  0.0058777 ,  0.00935818,  0.00331012]]),
                np.array([[-0.0126155 ,  0.00264827,  0.00886869,  0.01411371,  0.00518486],
                       [-0.01261795,  0.00264249,  0.00887132,  0.01410431,  0.00517832],
                       [-0.01261209,  0.00266095,  0.00885007,  0.01414492,  0.00520893],
                       [-0.01261059,  0.00265393,  0.00888564,  0.01410206,  0.00517435]]),
                np.array([[-0.01473925,  0.00305038,  0.01042287,  0.01648509,  0.00619734],
                       [-0.01472822,  0.00307533,  0.01042284,  0.01650903,  0.00621392],
                       [-0.01473146,  0.00307552,  0.01039554,  0.01653865,  0.00623778],
                       [-0.01472719,  0.00307848,  0.01041825,  0.01651621,  0.00621954]])
            ]

            feed = {
                'LSTM/WordEmbedding/SimpleLinear/Matrix:0': SOFTMAX_W,
                'LSTM/WordEmbedding/SimpleLinear/Bias:0': SOFTMAX_B,
                loss.targets: Y
            }

            for o,r in zip(rnn_outputs, m.rnn_outputs):
                feed[r] = o

            l = sess.run(loss.loss, feed)

            print(l)

            '''
[[-0.00115102 -0.01835673  0.01088401  0.00553839 -0.02548739  0.00961501
  -0.04911561  0.04094783  0.01729541  0.04113884  0.0110002   0.03410089
  -0.02663253  0.01714642  0.03581101 -0.03634553 -0.01540088 -0.01764538
   0.03884879 -0.03207963]
 [-0.00115117 -0.01835723  0.01088434  0.00553844 -0.02548673  0.00961541
  -0.04911538  0.04094752  0.01729532  0.04113849  0.01100097  0.0341017
  -0.02663185  0.01714566  0.03581182 -0.03634511 -0.0154006  -0.01764595
   0.03884758 -0.03208043]
 [-0.00115108 -0.01835609  0.01088368  0.00553811 -0.0254877   0.0096147
  -0.04911536  0.04094845  0.01729582  0.04113897  0.01099989  0.03410037
  -0.02663329  0.01714694  0.03581046 -0.03634582 -0.01540092 -0.01764458
   0.03884939 -0.03207891]
 [-0.0011517  -0.01835642  0.01088412  0.00553769 -0.02548616  0.00961538
  -0.0491141   0.0409487   0.01729641  0.04113809  0.01100182  0.03410203
  -0.02663257  0.01714548  0.03581202 -0.03634498 -0.01540009 -0.01764486
   0.03884656 -0.03208012]]
[[-0.00137119 -0.01813851  0.01110794  0.00582019 -0.02566941  0.00940851
  -0.04911464  0.04097762  0.0171818   0.04152314  0.01122282  0.0339342
  -0.02648103  0.01748628  0.03570804 -0.0365119  -0.01505298 -0.01722943
   0.03911369 -0.03211264]
 [-0.00137125 -0.01813885  0.01110811  0.00582023 -0.02566908  0.00940875
  -0.04911457  0.04097738  0.0171817   0.04152295  0.0112232   0.03393462
  -0.02648065  0.01748586  0.03570845 -0.03651169 -0.01505288 -0.01722979
   0.03911307 -0.03211308]
 [-0.00137074 -0.01813789  0.01110745  0.00582038 -0.02567078  0.0094078
  -0.04911549  0.04097777  0.01718157  0.04152391  0.01122116  0.03393264
  -0.02648198  0.01748771  0.03570654 -0.03651269 -0.01505364 -0.01722879
   0.03911621 -0.03211133]
 [-0.00137166 -0.01813823  0.01110794  0.00581962 -0.02566858  0.00940876
  -0.04911353  0.04097832  0.01718257  0.04152259  0.01122391  0.03393493
  -0.02648121  0.01748567  0.03570866 -0.03651157 -0.01505247 -0.01722896
   0.03911219 -0.03211286]]
[[-0.00148683 -0.01802572  0.01122714  0.00596118 -0.02575907  0.00930692
  -0.04911366  0.04099131  0.01712257  0.04171915  0.01133703  0.033848
  -0.02640488  0.01765947  0.03566255 -0.03659378 -0.01487576 -0.0170169
   0.03924932 -0.03213345]
 [-0.00148696 -0.01802452  0.01122649  0.00596062 -0.02575966  0.00930636
  -0.04911316  0.04099248  0.01712336  0.04171939  0.01133645  0.033847
  -0.02640637  0.01766046  0.03566148 -0.03659437 -0.01487585 -0.01701534
   0.03925047 -0.03213206]
 [-0.0014862  -0.01802442  0.01122626  0.00596135 -0.0257613   0.00930568
  -0.04911482  0.04099185  0.01712243  0.04172042  0.0113344   0.03384538
  -0.02640666  0.0176619   0.03566003 -0.03659511 -0.01487673 -0.0170155
   0.03925342 -0.03213113]
 [-0.00148684 -0.01802438  0.01122635  0.00596064 -0.02575997  0.0093062
  -0.04911336  0.04099251  0.01712332  0.04171955  0.01133605  0.03384664
  -0.02640661  0.01766078  0.03566112 -0.03659455 -0.01487602 -0.0170152
   0.03925106 -0.03213174]]
            '''

            assert False
